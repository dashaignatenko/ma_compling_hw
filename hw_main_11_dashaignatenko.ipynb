{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ ‚Ññ 11. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f21d5e",
      "metadata": {
        "id": "76f21d5e"
      },
      "source": [
        "### –ó–∞–¥–∞–Ω–∏–µ 1 (10 –±–∞–ª–ª–æ–≤).\n",
        "\n",
        "–í —Å–µ–º–∏–Ω–∞—Ä–µ –º—ã —Ä–∞–±–æ—Ç–∞–ª–∏ —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π alpaca –∏ dolly. –û–Ω–∏ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ. –í –¥–æ–º–∞—à–∫–µ –≤–∞–º –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∏ –æ–±—É—á–∏—Ç—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ —ç—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.\n",
        "–í –∫–∞—á–µ—Å—Ç–≤–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —É –≤–∞—Å –¥–æ–ª–∂–Ω–∞ –ø–æ–ª—É—á–∏—Ç—Å—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç —Å–≤—è–∑–Ω–æ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ü—Ä–∏–≤–µ–¥–∏—Ç–µ –∫–∞–∫ –º–∏–Ω–∏–º—É–º —Ç—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞. –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –Ω–µ —Ç–∞–∫ –≤–∞–∂–Ω–∞, —Ç–∞–∫ –∫–∞–∫ –≤—ã —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –±—É–¥–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏, –Ω–æ —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ —Ä–∞–Ω–¥–æ–º–Ω—ã–º.\n",
        "\n",
        "–†—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–æ–ª—å—à–µ 5 —Ç—ã—Å—è—á –ø—Ä–∏–º–µ—Ä–æ–≤. –û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–≤–∞–Ω –Ω–∞ alpaca/dolly (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≥–Ω–∞—Ç—å –≤—Å–µ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–Ω—É—é –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –Ω–∞ —Å–µ–º–∏–Ω–∞—Ä–µ, –∏–ª–∏ –¥–∞–∂–µ google translate). –ò–ª–∏ –≤—ã –º–æ–∂–µ—Ç–µ –ø—Ä–∏–¥—É–º–∞—Ç—å —Å–ø–æ—Å–æ–± —Å–æ–∑–¥–∞—Ç—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∫–∞–∫–∏–º-—Ç–æ –¥—Ä—É–≥–∏–º —Å–ø–æ—Å–æ–±–æ–º (–ø–µ—Ä–µ–¥–µ–ª–∞—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å –ø–æ–º–æ—â—å—é –ø—Ä–∞–≤–∏–ª). –î–∞—Ç–∞—Å–µ—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º, –º–æ–∂–Ω–æ —Å–∫–æ–æ–ø–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å –æ–¥–Ω–æ–≥—Ä—É–ø–ø–Ω–∏–∫–∞–º–∏ –∏ —Å–¥–µ–ª–∞—Ç—å –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –≤—Å–µ—Ö.\n",
        "\n",
        "–í—ã –º–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥–æ–æ–±—É—á–∞—Ç—å –ª—é–±—É—é –Ω–µ–±–æ–ª—å—à—É—é decoder-only –º–æ–¥–µ–ª—å. –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –ª—É—á—à–µ –≤—Å–µ–≥–æ –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª–∏, –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ (rugpt –Ω–∞–ø—Ä–∏–º–µ—Ä). –ù–æ –≤–æ–∑–º–æ–∂–Ω–æ –¥–∞–∂–µ –º–æ–¥–µ–ª–∏ –≤—Ä–æ–¥–µ opt –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –¥–æ–æ–±—É—á–∏—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö.\n",
        "\n",
        "–≠—Ç–æ –∑–∞–¥–∞–Ω–∏–µ –≥–æ—Ä–∞–∑–¥–Ω–æ –º–µ–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏. –ü–æ—ç—Ç–æ–º—É –Ω–µ —Å—Ç–µ—Å–Ω—è–π—Ç–µ—Å—å –∑–∞–¥–∞–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –≤ —á–∞—Ç–µ –∏–ª–∏ –ª–∏—á–Ω–æ, –µ—Å–ª–∏ —É –≤–∞—Å –≤–æ–∑–Ω–∏–∫–Ω—É—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG31ZpeXuIC6",
        "outputId": "e7b512e9-5e3c-464b-8aa6-ed04dabc1d21"
      },
      "id": "vG31ZpeXuIC6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWiarb-buIsR",
        "outputId": "74c26dc6-7731-4b67-8cb7-5be78f613575"
      },
      "id": "SWiarb-buIsR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.31.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install zstandard jsonlines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7Fs2B_7AhMK",
        "outputId": "611dfde7-ffd5-4136-85ac-79341ba8c70f"
      },
      "id": "K7Fs2B_7AhMK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "import json\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer"
      ],
      "metadata": {
        "id": "hcDzjUNtAglD"
      },
      "id": "hcDzjUNtAglD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate"
      ],
      "metadata": {
        "id": "6JDjWm51uq5h"
      },
      "id": "6JDjWm51uq5h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "527aa69a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "527aa69a",
        "outputId": "f80ca3dc-e1eb-4bb1-c782-accd8d77e84c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1491: FutureWarning: The repository for IlyaGusev/ru_turbo_alpaca contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/IlyaGusev/ru_turbo_alpaca\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data_alpaca = load_dataset(\"IlyaGusev/ru_turbo_alpaca\")\n",
        "\n",
        "#dataset = load_dataset(\"IlyaGusev/ru_turbo_saiga\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_alpaca['train'][:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGJTgI4sAW3T",
        "outputId": "2eae6367-575a-4a77-8da8-aa21ac571a1b"
      },
      "id": "PGJTgI4sAW3T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': ['–û–ø–∏—à–∏—Ç–µ, –∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å –≥–æ—Ä—à–æ–∫ –∏–∑ –≥–ª–∏–Ω—ã.',\n",
              "  '–û–ø–∏—à–∏ –ø—Ä–æ—Ü–µ—Å—Å –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è —Ç–æ—Ä—Ç–∞. –£–∫–∞–∂–∏ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ.',\n",
              "  '–ù–∞–ø–∏—à–∏ –∞–Ω–∞–ª–æ–≥ –¥–∞–Ω–Ω–æ–π –ø–æ—Å–ª–æ–≤–∏—Ü—ã –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ª–∞–¥: \"–ù–µ –≥–æ–≤–æ—Ä–∏ –≥–æ–ø, –ø–æ–∫–∞ –Ω–µ –ø–µ—Ä–µ–ø—Ä—ã–≥–Ω–µ—à—å\".'],\n",
              " 'input': ['', '', ''],\n",
              " 'output': ['–î–ª—è –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è –≥–æ—Ä—à–∫–∞ –∏–∑ –≥–ª–∏–Ω—ã —Å–Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤–∑—è—Ç—å –≥–ª–∏–Ω—É –∏ —Ä–∞–∑–º–µ—à–∞—Ç—å –µ–µ –≤–æ–¥–æ–π –¥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–ª–∞—Å—Ç–∏–ª–∏–Ω–∞. –ó–∞—Ç–µ–º –Ω—É–∂–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –≥–ª–∏–Ω—è–Ω—ã–π –∫–æ–º–æ–∫ –Ω—É–∂–Ω–æ–π —Ñ–æ—Ä–º—ã –∏ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–∞—á–∞—Ç—å –≤–ª–∞–≥—É –∏–∑ –≥–ª–∏–Ω—ã, —á—Ç–æ–±—ã –æ–Ω–∞ —Å—Ç–∞–ª–∞ —Ç–≤–µ—Ä–¥–æ–π. –ì–æ—Ä—à–æ–∫ –Ω—É–∂–Ω–æ –æ–±–∂–µ—á—å –≤ –¥—É—Ö–æ–≤–∫–µ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ, —á—Ç–æ–±—ã –æ–Ω —Å—Ç–∞–ª –ø—Ä–æ—á–Ω—ã–º –∏ –Ω–µ–ø—Ä–æ–Ω–∏—Ü–∞–µ–º—ã–º.',\n",
              "  '–î–ª—è –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è —Ç–æ—Ä—Ç–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã —Å–ª–µ–¥—É—é—â–∏–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã: –º—É–∫–∞, —Å–∞—Ö–∞—Ä, —è–π—Ü–∞, –º–æ–ª–æ–∫–æ, —Å–ª–∏–≤–æ—á–Ω–æ–µ –º–∞—Å–ª–æ, —Ä–∞–∑—Ä—ã—Ö–ª–∏—Ç–µ–ª—å, –≤–∞–Ω–∏–ª–∏–Ω. –°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ —Å–º–µ—à–∞—Ç—å –º—É–∫—É —Å —Å–∞—Ö–∞—Ä–æ–º –∏ –¥–æ–±–∞–≤–∏—Ç—å —è–π—Ü–∞, –º–æ–ª–æ–∫–æ, —Ä–∞–∑—Ä—ã—Ö–ª–∏—Ç–µ–ª—å –∏ –≤–∞–Ω–∏–ª—å. –ó–∞—Ç–µ–º —Ç–µ—Å—Ç–æ –Ω–∞–¥–æ —Ö–æ—Ä–æ—à–æ –ø–µ—Ä–µ–º–µ—à–∞—Ç—å –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–µ–π. –ö–∞–∂–¥—É—é —á–∞—Å—Ç—å –Ω—É–∂–Ω–æ –∑–∞–ø–µ–∫–∞—Ç—å –≤ –¥—É—Ö–æ–≤–∫–µ –¥–æ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏. –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ —Ç–æ—Ä—Ç –Ω–∞–¥–æ —Å–æ–±—Ä–∞—Ç—å, —Å–ª–æ–∂–∏–≤ —Å–ª–æ–∏ –ø–∏—Ä–æ–≥–∞ –≤–º–µ—Å—Ç–µ —Å –Ω–∞—á–∏–Ω–∫–æ–π –∏ —É–∫—Ä–∞—à–µ–Ω–∏—è–º–∏.',\n",
              "  '\"–ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–π —Å–≤–æ–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏, –ø–æ–∫–∞ –Ω–µ –±—É–¥–µ—Ç –Ω—É–∂–Ω–æ.\"'],\n",
              " 'alternative_output': ['–î–ª—è –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è –≥–æ—Ä—à–∫–∞ –∏–∑ –≥–ª–∏–Ω—ã –Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\\n\\n1. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –≥–ª–∏–Ω—É: –≥–ª–∏–Ω—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ö–æ—Ä–æ—à–æ –ø—Ä–æ–º—ã—Ç—å –∏ –∏–∑–º–µ–ª—å—á–∏—Ç—å. –ï—Å–ª–∏ –≥–ª–∏–Ω–∞ —Å–ª–∏—à–∫–æ–º —Å—É—Ö–∞—è, –µ–µ –Ω—É–∂–Ω–æ –∑–∞–º–æ—á–∏—Ç—å.\\n\\n2. –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –≥–æ—Ä—à–æ–∫: –≤–æ–∑—å–º–∏—Ç–µ –∫—É—Å–æ–∫ –≥–ª–∏–Ω—ã –∏ —Ä–∞–∑–º–∏–Ω–∞–π—Ç–µ –µ–≥–æ –≤ —Ä—É–∫–∞—Ö, —Å–¥–µ–ª–∞–≤ —à–∞—Ä–∏–∫. –ü–æ—Ç–æ–º –Ω–∞—á–∏–Ω–∞–π—Ç–µ –≤—Ä–∞—â–∞—Ç—å —à–∞—Ä–∏–∫ –º–µ–∂–¥—É –ª–∞–¥–æ–Ω—è–º–∏, —á—Ç–æ–± —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª—Å—è —Ü–∏–ª–∏–Ω–¥—Ä. –ó–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞–ª—å—Ü—ã, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Å—Ç–µ–Ω–∫–∏ –≥–æ—Ä—à–∫–∞ –≤—ã—Å–æ—Ç–æ–π –∏ –¥–∏–∞–º–µ—Ç—Ä–æ–º –ø–æ –∂–µ–ª–∞–Ω–∏—é.\\n\\n3. –°–¥–µ–ª–∞—Ç—å –æ—Ç–≤–µ—Ä—Å—Ç–∏–µ –¥–ª—è –¥—Ä–µ–Ω–∞–∂–∞: –≤—ã —Å–º–æ–∂–µ—Ç–µ —Å–¥–µ–ª–∞—Ç—å –æ—Ç–≤–µ—Ä—Å—Ç–∏–µ –¥–ª—è –¥—Ä–µ–Ω–∞–∂–∞ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –≤–æ–¥–∞ —Å–º–æ–≥–ª–∞ –≤—ã—Ö–æ–¥–∏—Ç—å –∏–∑ –≥–æ—Ä—à–∫–∞, –∫–æ–≥–¥–∞ –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –ø–æ–¥—Å—Ç–∞–≤–∫–µ. –î–ª—è —ç—Ç–æ–≥–æ, –≤–æ—Å–ø–æ–ª—å–∑—É–π—Ç–µ—Å—å –Ω–æ–∂–æ–º –∏–ª–∏ –¥—Ä—É–≥–∏–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –¥–µ–ª–∞–Ω–∏—è –æ—Ç–≤–µ—Ä—Å—Ç–∏—è –Ω–∞ –¥–Ω–µ –≥–æ—Ä—à–∫–∞.\\n\\n4. –í—ã—Å—É—à–∏—Ç—å –≥–æ—Ä—à–æ–∫: –¥–∞–π—Ç–µ –≥–æ—Ä—à–∫—É –≤—ã—Å–æ—Ö–Ω—É—Ç—å –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ 24 —á–∞—Å–æ–≤ –ø—Ä–∏ –∫–æ–º–Ω–∞—Ç–Ω–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ.\\n\\n5. –û–±–∂–∏–≥ –≥–æ—Ä—à–∫–∞: –ø–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ –≥–æ—Ä—à–æ–∫ –≤—ã—Å–æ—Ö–Ω–µ—Ç, —Å–ª–µ–¥—É–µ—Ç –æ–±–∂–µ—á—å –µ–≥–æ –Ω–∞ —Ç–µ—Ä–º–æ—Å—Ç–æ–π–∫–æ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏, —Ç–∞–∫–æ–π –∫–∞–∫ –ø–ª–∞–≤–ª–µ–Ω–Ω–æ–µ —Å—Ç–µ–∫–ª–æ –∏–ª–∏ –∫–∏—Ä–ø–∏—á. –≠—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —á—Ç–æ–±—ã –≥–æ—Ä—à–æ–∫ —Å—Ç–∞–ª –∫—Ä–µ–ø—á–µ. –û–±–∂–∏–≥–∞–π—Ç–µ –≥–æ—Ä—à–æ–∫ –≤ –ø–µ—á–∏ –Ω–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ 900 –≥—Ä–∞–¥—É—Å–æ–≤ –¶–µ–ª—å—Å–∏—è –≤ —Ç–µ—á–µ–Ω–∏–µ 3-4 —á–∞—Å–æ–≤. \\n\\n6. –†–∞—Å–∫—Ä–∞—à–∏–≤–∞–Ω–∏–µ: –ø–æ –∂–µ–ª–∞–Ω–∏—é, –º–æ–∂–Ω–æ —Ä–∞—Å–∫—Ä–∞—Å–∏—Ç—å –≥–æ—Ä—à–æ–∫ –∏ –ø–æ–∫—Ä—ã—Ç—å –ª–∞–∫–æ–º.\\n\\n–ì–æ—Ç–æ–≤! –¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –≥–æ—Ä—à–æ–∫.',\n",
              "  '–î–ª—è –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è —Ç–æ—Ä—Ç–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–ª–µ–¥—É—é—â–µ–µ:\\n\\n–ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∞:\\n\\n- –ú—É–∫–∞\\n- –°–∞—Ö–∞—Ä\\n- –Ø–π—Ü–∞\\n- –†–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –º–∞—Å–ª–æ –∏–ª–∏ —Å–ª–∏–≤–æ—á–Ω–æ–µ –º–∞—Å–ª–æ\\n- –†–∞–∑—Ä—ã—Ö–ª–∏—Ç–µ–ª—å, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, —ç—Ç–æ –ø–æ—Ä–æ—à–æ–∫ –¥–ª—è –≤—ã–ø–µ—á–∫–∏\\n- –ú–æ–ª–æ–∫–æ\\n\\n–ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –¥–ª—è –∫—Ä–µ–º–∞:\\n\\n- –°–ª–∏–≤–æ—á–Ω–æ–µ –º–∞—Å–ª–æ\\n- –°–∞—Ö–∞—Ä–Ω–∞—è –ø—É–¥—Ä–∞\\n- –í–∞–Ω–∏–ª–∏–Ω –∏–ª–∏ –¥—Ä—É–≥–æ–π –∞—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä\\n- –ú–æ–ª–æ–∫–æ –∏–ª–∏ —Å–ª–∏–≤–∫–∏\\n- –§—Ä—É–∫—Ç—ã –∏–ª–∏ —è–≥–æ–¥—ã (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è)\\n\\n–®–∞–≥–∏ –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è —Ç–æ—Ä—Ç–∞:\\n\\n1. –î–ª—è —Ç–µ—Å—Ç–∞ —Å–º–µ—à–∞–π—Ç–µ –º—É–∫—É, —Å–∞—Ö–∞—Ä –∏ —Ä–∞–∑—Ä—ã—Ö–ª–∏—Ç–µ–ª—å –≤ –æ–¥–Ω–æ–π –±–æ–ª—å—à–æ–π –º–∏—Å–∫–µ.\\n\\n2. –û—Ç–¥–µ–ª—å–Ω–æ –≤–∑–±–µ–π—Ç–µ —è–π—Ü–∞ –∏ –¥–æ–±–∞–≤—å—Ç–µ –∏—Ö –≤ –º–∏—Å–∫—É —Å —Å—É—Ö–∏–º–∏ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º–∏. –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è–π—Ç–µ –º–æ–ª–æ–∫–æ –∏ –º–∞—Å–ª–æ.\\n\\n3. –í—ã–ª–µ–π—Ç–µ —Ç–µ—Å—Ç–æ –≤ —Ä–∞–∑–æ–≥—Ä–µ—Ç—É—é –¥—É—Ö–æ–≤–∫—É –∏ –∏—Å–ø–µ–∫–∏—Ç–µ –¥–æ –∑–æ–ª–æ—Ç–∏—Å—Ç–æ–π –∫–æ—Ä–æ—á–∫–∏.\\n\\n4. –î–ª—è –∫—Ä–µ–º–∞ –≤–∑–±–µ–π—Ç–µ —Å–ª–∏–≤–æ—á–Ω–æ–µ –º–∞—Å–ª–æ, –¥–æ–±–∞–≤—å—Ç–µ —Å–∞—Ö–∞—Ä–Ω—É—é –ø—É–¥—Ä—É –∏ –∞—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä. –ï—Å–ª–∏ –∫—Ä–µ–º —Å–ª–∏—à–∫–æ–º –≥—É—Å—Ç–æ–π, –¥–æ–±–∞–≤—å—Ç–µ –º–æ–ª–æ–∫–æ –∏–ª–∏ —Å–ª–∏–≤–∫–∏.\\n\\n5. –†–∞–∑—Ä–µ–∂—å—Ç–µ —Ç–æ—Ä—Ç –Ω–∞ –¥–≤–µ —á–∞—Å—Ç–∏ –∏ –Ω–∞–ª–µ–π—Ç–µ –∫—Ä–µ–º –º–µ–∂–¥—É —Å–ª–æ—è–º–∏. –£–∫—Ä–∞—Å—å—Ç–µ —Ç–æ—Ä—Ç —Ñ—Ä—É–∫—Ç–∞–º–∏ –∏–ª–∏ —è–≥–æ–¥–∞–º–∏.\\n\\n6. –ù–∞—Ö–æ–¥–∏—Ç–µ —É—é—Ç–Ω–æ–µ –º–µ—Å—Ç–æ –∏ –Ω–∞—Å–ª–∞–∂–¥–∞–π—Ç–µ—Å—å –≤–∫—É—Å–æ–º —Å–≤–æ–µ–≥–æ —Ç–æ—Ä—Ç–∞.',\n",
              "  '\"–ù–µ –¥–µ–ª–∞–π –æ–±–µ—â–∞–Ω–∏–π, –ø–æ–∫–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ —É–≤–µ—Ä–µ–Ω –≤ —Å–≤–æ–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö\".'],\n",
              " 'label': ['bad_output', None, None],\n",
              " 'all_labels': [['bad_output', 'bad_output'], [], []],\n",
              " 'agreement': [1.0, None, None],\n",
              " 'overlap': [2, None, None]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "NOEdQmKGWe-G"
      },
      "id": "NOEdQmKGWe-G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )"
      ],
      "metadata": {
        "id": "nQgRSWmPWfAf"
      },
      "id": "nQgRSWmPWfAf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)"
      ],
      "metadata": {
        "id": "FayJpE3zWh7Q"
      },
      "id": "FayJpE3zWh7Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class SupervisedDataset(Dataset):\n",
        "#     \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "\n",
        "#     def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "#         super(SupervisedDataset, self).__init__()\n",
        "#         logging.warning(\"Loading data...\")\n",
        "#         list_data_dict = json.load(open(data_path))\n",
        "\n",
        "#         logging.warning(\"Formatting inputs...\")\n",
        "#         prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
        "#         sources = [\n",
        "#             prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "#             for example in list_data_dict\n",
        "#         ]\n",
        "#         targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
        "\n",
        "#         logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "#         data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "#         self.input_ids = data_dict[\"input_ids\"]\n",
        "#         self.labels = data_dict[\"labels\"]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.input_ids)\n",
        "\n",
        "#     def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "#         return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
      ],
      "metadata": {
        "id": "ytxdOHCostw-"
      },
      "id": "ytxdOHCostw-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, data, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        list_data_dict = data[\"train\"]\n",
        "\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
        "        sources = [\n",
        "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "            for example in list_data_dict\n",
        "        ]\n",
        "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ],
      "metadata": {
        "id": "lnE5w8vKW46S"
      },
      "id": "lnE5w8vKW46S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = 'facebook/opt-350m'\n",
        "model_name = \"facebook/opt-125m\"\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        max_length=512,\n",
        "        cache_dir=\"huggingface_cache\",\n",
        "    )"
      ],
      "metadata": {
        "id": "keGTRSlzrh-k"
      },
      "id": "keGTRSlzrh-k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=\"huggingface_cache\",\n",
        "    model_max_length=512,\n",
        "    padding_side=\"right\",\n",
        "    use_fast=False,\n",
        ")"
      ],
      "metadata": {
        "id": "FQEIsfrKriFB"
      },
      "id": "FQEIsfrKriFB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SupervisedDataset(tokenizer=tokenizer, data=data_alpaca)\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goPyPOaor6Xc",
        "outputId": "203e1db4-96a5-42b9-819f-0ed26571752f"
      },
      "id": "goPyPOaor6Xc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:Formatting inputs...\n",
            "WARNING:root:Tokenizing inputs... This may take some time...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = transformers.TrainingArguments(learning_rate=1e-5,\n",
        "                 num_train_epochs=1,\n",
        "                 per_device_train_batch_size=2,\n",
        "                 gradient_accumulation_steps=1,\n",
        "                 evaluation_strategy='no',\n",
        "                 weight_decay=0.,\n",
        "                 warmup_ratio=0.03,\n",
        "                 lr_scheduler_type=\"cosine\",\n",
        "                 save_strategy='no',\n",
        "                 logging_steps=1000,\n",
        "                 output_dir=\"opt125_instruct_ft\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN2VaiKdr6aA",
        "outputId": "aa2a0cfa-5f90-4d66-a093-4766dcc9be7b"
      },
      "id": "GN2VaiKdr6aA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model,\n",
        "                 tokenizer=tokenizer,\n",
        "                 args=train_args,\n",
        "                 train_dataset=train_dataset,\n",
        "                 eval_dataset=None,\n",
        "                 data_collator=data_collator)"
      ],
      "metadata": {
        "id": "mFVjyQBsvA9_"
      },
      "id": "mFVjyQBsvA9_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "qf1Wv4NjvBAl",
        "outputId": "bac6bb81-0b76-4f69-aced-77b3b1d1c8e8"
      },
      "id": "qf1Wv4NjvBAl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14911' max='14911' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14911/14911 1:10:55, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.268900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.176900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.114800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.095700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.073800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.045500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.032300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.017200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.993500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.987700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.984400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.976000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.979400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=14911, training_loss=1.0486835374589754, metrics={'train_runtime': 4257.5852, 'train_samples_per_second': 7.004, 'train_steps_per_second': 3.502, 'total_flos': 6836609053440000.0, 'train_loss': 1.0486835374589754, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('opt125_ft_02')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyyYVr0IvBDM",
        "outputId": "d489401d-486e-4f27-c4a1-fcc7fc624eb6"
      },
      "id": "WyyYVr0IvBDM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "M_NQa_bFr6co"
      },
      "id": "M_NQa_bFr6co",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'opt125_ft_02'"
      ],
      "metadata": {
        "id": "ERMMeEmVriHV"
      },
      "id": "ERMMeEmVriHV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512, max_length=512)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, max_length=512)"
      ],
      "metadata": {
        "id": "HAMcvAWDvO6s"
      },
      "id": "HAMcvAWDvO6s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_for_instruction(instruction, text, model):\n",
        "    text = text.replace('\\n', ' ')\n",
        "    prompt = (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "              \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "              f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{text}\\n\\n### Response:\")\n",
        "\n",
        "    inputs = tokenizer([prompt],\n",
        "                        return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "        # this parameters are also important but you can read about them in the docs and just try changing them\n",
        "       # num_beams=1,\n",
        "      #  temperature=0.4,\n",
        "        max_length=1500,\n",
        "        no_repeat_ngram_size=10,\n",
        "        repetition_penalty= 5.0,\n",
        "    #     length_penalty=0.01,\n",
        "         early_stopping=True,\n",
        "    #    do_sample=True,\n",
        "    #     top_k=30,\n",
        "    #     top_p=0.8,\n",
        "     #   early_stopping=False,\n",
        "    #     num_return_sequences=3,\n",
        "        num_return_sequences= 1,\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        do_sample=False,  # disable sampling to test if batching affects output\n",
        "    )\n",
        "    summaries = tokenizer.batch_decode(output_sequences[:,len(inputs[0]):], skip_special_tokens=True)\n",
        "    return summaries[0]"
      ],
      "metadata": {
        "id": "tnzeKNgevvKZ"
      },
      "id": "tnzeKNgevvKZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_for_instruction(instruction, text, model):\n",
        "    text = text.replace('\\n', ' ')\n",
        "    prompt = (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "              \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "              f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{text}\\n\\n### Response:\")\n",
        "\n",
        "    inputs = tokenizer([prompt],\n",
        "                        return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "        # this parameters are also important but you can read about them in the docs and just try changing them\n",
        "        num_beams=1,\n",
        "#         temperature=0.4,\n",
        "         max_length=1000,\n",
        "  #      max_new_tokens=80,\n",
        "#         no_repeat_ngram_size=3,\n",
        "    #     repetition_penalty= 5.0,\n",
        "    #     length_penalty=0.01,\n",
        "    #     early_stopping=True,\n",
        "    #     do_sample=True,\n",
        "    #     top_k=30,\n",
        "    #     top_p=0.8,\n",
        "        early_stopping=True,\n",
        "    #     num_return_sequences=3,\n",
        "        num_return_sequences= 1,\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        do_sample=True,  # disable sampling to test if batching affects output\n",
        "    )\n",
        "    summaries = tokenizer.batch_decode(output_sequences[:,len(inputs[0]):], skip_special_tokens=True)\n",
        "    return summaries[0]"
      ],
      "metadata": {
        "id": "QL0ZiGokEzgu"
      },
      "id": "QL0ZiGokEzgu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "–ò–∑–≤–µ—Å—Ç–Ω–æ–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –ø–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞–ª–æ –Ω–∞ —Å–º–æ—Ç—Ä–∏—Ç–µ–ª—è —É—Å—ã–ø–ª—è—é—â–µ. –û–Ω –∫–∞–∫ –±—ã\n",
        "—Å—Ä–∞–∑—É –∑–∞—Å–Ω—É–ª, –ø–µ—Ä–µ—Å—Ç–∞–≤ –±–æ—Ä–º–æ—Ç–∞—Ç—å –∏ –æ–ø—Ä–∞–≤–¥—ã–≤–∞—Ç—å—Å—è. –ï–≥–æ —Å–ª–µ–≥–∫–∞ —Å–æ–≥–Ω—É—Ç–∞—è –≤ –ø–æ—è—Å–Ω–∏—Ü–µ —Ñ–∏–≥—É—Ä–∞ –≤\n",
        "–∫–æ—Ä–æ—Ç–∫–æ–π –¥—É—à–µ–≥—Ä–µ–π–∫–µ, –ø–ª—é—à–µ–≤—ã—Ö —à—Ç–∞–Ω–∞—Ö –∏ –≤—ã—Å–æ–∫–∏—Ö –±–µ–ª—ã—Ö, –ø–æ–¥—à–∏—Ç—ã—Ö –∂–µ–ª—Ç–æ–π –∫–æ–∂–µ–π –≤–∞–ª–µ–Ω–∫–∞—Ö –∑–∞—Å—Ç—ã–ª–∞ –Ω–µ–ø–æ–¥–≤–∏–∂–Ω–æ –≤ —Å—É–º—Ä–∞–∫–µ –ø—Ä–æ—Å—Ç–æ—Ä–Ω–æ–π, —Å–∏–ª—å–Ω–æ –Ω–∞—Ç–æ–ø–ª–µ–Ω–Ω–æ–π –≥–æ—Ä–Ω–∏—Ü—ã. –ó–∞—Ç–æ –µ–≥–æ –∂–µ–Ω–∞, —Ç–∏—Ö–æ –¥–æ —ç—Ç–æ–≥–æ —Å–∏–¥–µ–≤—à–∞—è —Å –≤—è–∑–∞–Ω—å–µ–º –≤ –¥–∞–ª—å–Ω–µ–º —É–≥–ª—É –∑–∞ —Å–∏—Ç—Ü–µ–≤–æ–π –∑–∞–Ω–∞–≤–µ—Å–∫–æ–π, –∑–∞–≤–æ—Ä–æ—á–∞–ª–∞—Å—å, –≤—ã–≥–ª—è–Ω—É–ª–∞, –ø–æ–∫–∞–∑—ã–≤–∞—è —Å–≤–æ–µ —à–∏—Ä–æ–∫–æ–µ, –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã—Ä–∞–∂–∞—é—â–µ–µ –ª–∏—Ü–æ, —É–∂–µ —É—Å–ø–µ–≤—à–µ–µ –æ—Å—Ç–æ—á–µ—Ä—Ç–µ—Ç—å –¥–æ–∫—Ç–æ—Ä—É –∑–∞ —ç—Ç–∏ –¥–≤–∞ —á–∞—Å–∞ –æ–∂–∏–¥–∞–Ω–∏—è, –ø–∏—Ç–∏—è —á–∞—è —Å –º–∞–ª–∏–Ω–æ–≤—ã–º –∏ —Å–ª–∏–≤–æ–≤—ã–º –≤–∞—Ä–µ–Ω—å–µ–º –∏ –ª–∏—Å—Ç–∞–Ω–∏—è –ø—Ä–æ—à–ª–æ–≥–æ–¥–Ω–µ–π ¬´–ù–∏–≤—ã¬ª\"\"\""
      ],
      "metadata": {
        "id": "Tk8wOMiqBP0a"
      },
      "id": "Tk8wOMiqBP0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"–°–æ–∫—Ä–∞—Ç–∏ —Ç–µ–∫—Å—Ç.\"\n",
        "predict_for_instruction(instruction, text, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "qLgn1EgYF5PK",
        "outputId": "7c8f2af4-b33b-4c15-dac7-a385f449a3a9"
      },
      "id": "qLgn1EgYF5PK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'–ù–∞ —Å–º–æ—Ç—Ä–∏—Ç–µ–ª—è —É—Å—ã–ø–ª—è—é—â–µ –ø–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞–ª–æ –Ω–∞ —Å–º–æ—Ç—Ä–∏—Ç–µ–ª—è –ø–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞–ª–æ, –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω–æ, —á—Ç–æ –≤ —Å–º–æ—Ç—Ä–∏—Ç–µ–ª—è –∫–æ–∂–µ–π –ø–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞–ª–æ –Ω–∞—Ö–æ–¥–∏—Ç—å –≤—ã—Å–æ–∫–∏–µ –∏ –∏—Å–Ω–æ—à–µ–Ω–∏—è. –ü–æ–¥–∞–≤–∞–π—Ç–µ—Å—å —É—Å—ã–ø—å—é –ø—Ä–æ–¥–ª–∏—Ç–µ–ª—å–Ω—ã–º –ø—Ä–æ—Å—Ç–æ–º –≤ –¥–∞–≤–ª–µ–Ω–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∂–∏–∑–Ω—å –∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å–Ω–æ, –µ—Å–ª–∏ –≤—ã –æ–∫–∞–∑'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "—Ç–µ–∫—Å—Ç –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏–ª—Å—è, –Ω–æ..."
      ],
      "metadata": {
        "id": "Eib9uTn3GWJn"
      },
      "id": "Eib9uTn3GWJn"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wNIXJUyIvvNC"
      },
      "id": "wNIXJUyIvvNC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"–†–∞—Å—Å–∫–∞–∂–∏ –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ö–æ—Ä–æ—à–µ–≥–æ —Ä–µ–∑—é–º–µ.\"\n",
        "predict_for_instruction(instruction, text, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "erDR88mCvvPY",
        "outputId": "3f9481d1-60dc-4955-aa35-9921007fa3b3"
      },
      "id": "erDR88mCvvPY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'–ö–æ–≥–¥–∞ –ª—é–¥–∏ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω—ã, –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –æ–ø–∞—Å–Ω–æ–µ –º–µ—Å—Ç–æ? –ö –≤–∫—É—Å–Ω—ã–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º —Ä–µ–∑—é–º–µ, –≤–∞–º –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤–∫–ª—é—á–∞—Ç—å –ø–æ—á—Ç—ã? –ö –Ω–∞—à—É –ø–µ—Ä—Ü—É –æ–Ω –±—ã–ª –±–æ–ª–µ–µ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–º.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "‚à´(3x¬≤ - 2x + 1)dx\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DtjykEcSHc7M"
      },
      "id": "DtjykEcSHc7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"–†–µ—à–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–π –∏–Ω—Ç–µ–≥—Ä–∞–ª:\"\n",
        "predict_for_instruction(instruction, text, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bpjmIASaHc9y",
        "outputId": "3499277c-1453-4b80-e037-c7465c1683f9"
      },
      "id": "bpjmIASaHc9y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'x=y^2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í —Ü–µ–ª–æ–º –º–æ–¥–µ–ª—å —É–∂–∞—Å–Ω–æ –±—Ä–µ–¥–∏—Ç, –Ω–æ –∫–∞–∫ –±—É–¥—Ç–æ –∫ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø—ã—Ç–∞–µ—Ç—Å—è –ø—Ä–∏—Å–ª—É—à–∞—Ç—å—Å—è))"
      ],
      "metadata": {
        "id": "fGSgHOUAHuF5"
      },
      "id": "fGSgHOUAHuF5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}