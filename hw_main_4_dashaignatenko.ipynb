{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39c2056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг \\<start>  \n",
    "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хронить биграмы, а по колонкам униграммы \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
    "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cca2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvach = open('data/2ch_corpus.txt').read()\n",
    "news = open('data/lenta.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "6fc5aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "7c42aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "72aab7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#вот тут, наверное, добавить еще один тег <start>? \n",
    "#start0 чтобы после него точно шел только start, а после start только слово (а не новый тег start)\n",
    "sentences_dvach = [['<start0>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach[:5000000])]\n",
    "sentences_news = [['<start0>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news[:5000000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d81c6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence))\n",
    "    trigrams_dvach.update(ngrammer(sentence, n=3))\n",
    "\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dde3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix, csr_matrix, csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "b0bdadd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# матрица слова на слова (инициализируем нулями)\n",
    "matrix_dvach = lil_matrix((len(bigrams_dvach), \n",
    "                         len(unigrams_dvach)))\n",
    "#индексы для униграм и биграм\n",
    "id2bigr_dvach = list(bigrams_dvach)\n",
    "bigr2id_dvach = {bigram:i for i, bigram in enumerate(id2bigr_dvach)}\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "\n",
    "\n",
    "# заполняем матрицу\n",
    "for ngram in trigrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = ' '.join([word1, word2])\n",
    "    # на пересечение биграмы и слова ставим вероятность встретить второе после первого\n",
    "    matrix_dvach[bigr2id_dvach[bigram], word2id_dvach[word3]] =  (trigrams_dvach[ngram]/\n",
    "                                                                     bigrams_dvach[bigram])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "41655379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame.sparse.from_spmatrix(matrix_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3767eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# то же самое для другого корпуса\n",
    "# матрица слова на слова (инициализируем нулями)\n",
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                         len(unigrams_news)))\n",
    "\n",
    "#индексы для униграм и биграм\n",
    "id2bigr_news = list(bigrams_news)\n",
    "bigr2id_news = {bigram:i for i, bigram in enumerate(id2bigr_news)}\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = ' '.join([word1, word2])\n",
    "    # на пересечении биграмы и слова ставим вероятность встретить второе после первого\n",
    "    matrix_news[bigr2id_news[bigram], word2id_news[word3]] =  (trigrams_news[ngram]/\n",
    "                                                                     bigrams_news[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21678695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "84f303de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(matrix, id2word, id2bigr, bigr2id, n=100, start='<start0> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigr2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "#         p = matrix[current_idx].toarray()[0]\n",
    "#         p = np.asarray(p).astype('float64')\n",
    "#         p = p / np.sum(p)\n",
    "#         chosen = np.random.choice(matrix.shape[1], p=p)\n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
    "        text.append(id2word[chosen]) \n",
    "        #из последнего слова предыдущей биграмы и добавленного слова делаем новую биграму, получаем новый current_idx\n",
    "        chosen_bg = bigr2id[' '.join([id2bigr[current_idx].split()[1], id2word[chosen]])]\n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen_bg = bigr2id['<start0> <start>']\n",
    "        \n",
    "        current_idx = chosen_bg\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d68447bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "оптимизон нормальный это такая штука когда тебе в общем-то нихуя \n",
      " с 2009 в завязке \n",
      " ночью вообще все тян не хорошие лол \n",
      " а сколько время то \n",
      " опиши стек для стандартных gui приложений winforms в большинстве городов но это малое от чего умер \n",
      " допустим у меня по 30 лет однозначно уверен что дело в том что сами за 5 к выкинул просто на основе шариата принимает \n",
      " всмысле ты употребляешь говно \n",
      " а есть пидоры \n",
      " давайте я у него в открытом космосе летают мириады мелких объектов по совершенно непредсказуемым орбитам \n",
      " ты в толчок стучался постоянно\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach, id2bigr_dvach, bigr2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3d026c",
   "metadata": {},
   "source": [
    "## для трех слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4343a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start0>'] + ['<start1>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach[:5000000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3b6cef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "fourgrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence))\n",
    "    trigrams_dvach.update(ngrammer(sentence, n=3))\n",
    "    fourgrams_dvach.update(ngrammer(sentence, n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "48b57fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# матрица слова на слова (инициализируем нулями)\n",
    "matrix_dvach = lil_matrix((len(trigrams_dvach), \n",
    "                         len(unigrams_dvach)))\n",
    "#индексы для униграм и биграм\n",
    "id2trigr_dvach = list(trigrams_dvach)\n",
    "trigr2id_dvach = {tg:i for i, tg in enumerate(id2trigr_dvach)}\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "\n",
    "\n",
    "# заполняем матрицу\n",
    "for ngram in fourgrams_dvach:\n",
    "    word1, word2, word3, word4 = ngram.split()\n",
    "    trigram = ' '.join([word1, word2, word3])\n",
    "    # на пересечении триграмы и слова ставим вероятность встретить второе после первого\n",
    "    matrix_dvach[trigr2id_dvach[trigram], word2id_dvach[word4]] =  (fourgrams_dvach[ngram]/\n",
    "                                                                     trigrams_dvach[trigram])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "28b39dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, id2trigr, trigr2id, n=100, start='<start0> <start1> <start>'):\n",
    "    text = []\n",
    "    current_idx = trigr2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
    "        text.append(id2word[chosen]) \n",
    "        #из последних двух слов предыдущей 3грамы и добавленного слова делаем новую триграму, получаем новый current_idx\n",
    "        chosen_bg = trigr2id[' '.join([id2trigr[current_idx].split()[1], id2trigr[current_idx].split()[2], id2word[chosen]])]\n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen_bg = trigr2id[start]\n",
    "        current_idx = chosen_bg\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1048bbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я там не был ни разу \n",
      " как вообще психонавт под лсд выглядит со стороны \n",
      " почему тебе нечего ответить гуманитарий \n",
      " сейчас хочу окончательно спрыгнуть хуй знает получится ли \n",
      " деды за хлеб воевали а она хлеб в пизду суёт и не уважает его ну вообще сувать в пизду член пиздец какой порок штоле мне кажется тебе максимум 16 \n",
      " только не это лучше уж жоповирусный чем ротавирусный \n",
      " я как то загорелся его посмотреть залез на свой любимый рутрекер а там нихуя \n",
      " и если честно то я бы его только обоссал \n",
      " ведь строить заново ничего не\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach, id2trigr_dvach, trigr2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9ba8939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ты то злой то печальный то ты в предвкушении чуда то чуть не плачешь от своей безнадежности \n",
      " тебя спросили про теракты в кса а ты ответил про теракты в турции \n",
      " я про то что это мировая тенденция продавленная мировой плутократией еще не означает что он тупой скорее наоборот умный и богатый \n",
      " но вот вам парочка советов \n",
      " • у человека 23 пары хромосом \n",
      " не твоя вот и бесишься \n",
      " пока не решили \n",
      " — isbn 966-00-0604-7 \n",
      " даже если есть такой вариант через нейтральные системы кругом из 2-3 систем вполне вариант обойти \n",
      " меня вообще удивляет\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach, id2trigr_dvach, trigr2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1bb3a9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['я сперва пожарил рис минуту в масле на сковороде так что зёрнышки разделились и чутка пожелтели потом добавил три объёма воды и на маленькой скорости варил десять минут ',\n",
       " ' просит чтоб опять выходили а то может ты мне так же пытаешься это объяснить и мы оба друг друг не понимаем ',\n",
       " ' бывает у некоторых вообще ульт убивает все что увидит и они это пытаются выдать за киберспортивную дисциплину вывод лучше бы они начали разработку 4 варика или нового старкрафта или хотя бы первый шаг сделать начать работать и зарабатывать ',\n",
       " ' я был бы не против поменять кое-какие безделицы на патроны']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = generate(matrix_dvach, id2word_dvach, id2trigr_dvach, trigr2id_dvach).split('<end>')\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc79d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#довольно связные тексты)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7e546",
   "metadata": {},
   "source": [
    "## перплексия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b120172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#по двум предыдущим словам\n",
    "def compute_join_proba_markov_assumption(phrase, bigram_counts, trigram_counts):\n",
    "    prob = 0\n",
    "    tokens = normalize(phrase)\n",
    "    for ngram in ngrammer(['<start0>'] + ['<start>'] + tokens + ['<end>'], n=3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = ' '.join([word1, word2])\n",
    "        if bigram in bigram_counts and ngram in trigram_counts:\n",
    "            prob += np.log(trigram_counts[ngram]/bigram_counts[bigram])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "2619d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#униграмная для сравнения\n",
    "def compute_joint_proba(text, word_probas):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for word in tokens:\n",
    "        if word in word_probas:\n",
    "            prob += (np.log(word_probas[word]))\n",
    "        else:\n",
    "            prob += np.log(2e-4)\n",
    "    \n",
    "    return prob, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "6d756bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dvach = normalize(dvach)\n",
    "vocab_dvach = Counter(norm_dvach)\n",
    "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "d79e45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "7c645a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62601265.468576446"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#триграмная\n",
    "ps = []\n",
    "for sent in sent_tokenize(dvach[-3000:]):\n",
    "    prob, N = compute_join_proba_markov_assumption(sent, bigrams_dvach, trigrams_dvach)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps.append(perplexity(prob, N))\n",
    "\n",
    "np.mean(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13664c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#очень высокая перплексия...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "d56dca6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12046.868014234864"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#униграмная\n",
    "ps = []\n",
    "for sent in sent_tokenize(dvach[-3000:]):\n",
    "    prob, N = compute_joint_proba(sent, probas_dvach)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps.append(perplexity(prob, N))\n",
    "np.mean(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3806d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733858c",
   "metadata": {},
   "source": [
    "Измените функцию generate_with_beam_search так, чтобы она работала с моделью, которая учитывает два предыдущих слова. \n",
    "Сравните получаемый результат с первым заданием. \n",
    "Также попробуйте начинать генерацию не с нуля (подавая \\<start> \\<start>), а с какого-то промпта. Но помните, что учитываться будут только два последних слова, так что не делайте длинные промпты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "7ffde3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news = [['<start0>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news[:5000000])]\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "0afdc5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# матрица слова на слова (инициализируем нулями)\n",
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                         len(unigrams_news)))\n",
    "\n",
    "#индексы для униграм и биграм\n",
    "id2bigr_news = list(bigrams_news)\n",
    "bigr2id_news = {bigram:i for i, bigram in enumerate(id2bigr_news)}\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = ' '.join([word1, word2])\n",
    "    # на пересечении биграмы и слова ставим вероятность встретить второе после первого\n",
    "    matrix_news[bigr2id_news[bigram], word2id_news[word3]] =  (trigrams_news[ngram]/\n",
    "                                                                     bigrams_news[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "c426746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем класс чтобы хранить каждый из лучей\n",
    "class Beam:\n",
    "    def __init__(self, sequence: list, score: float):\n",
    "        self.sequence: list = sequence\n",
    "        self.score: float = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "f2508f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_beam_search(matrix, bigr2id, id2word, word2id, n=100, max_beams=5, start='<start0> <start>'):\n",
    "    # изначально у нас один луч с заданным началом (start по дефолту)\n",
    "    initial_node = Beam(sequence=start.split(), score=np.log1p(0))\n",
    "    beams = [initial_node]\n",
    "    \n",
    "    for i in range(n):\n",
    "        # делаем n шагов генерации\n",
    "        new_beams = []\n",
    "        # на каждом шаге продолжаем каждый из имеющихся лучей\n",
    "        for beam in beams:\n",
    "            # лучи которые уже закончены не продолжаем (но и не удаляем)\n",
    "            if beam.sequence[-1] == '<end>':\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "            \n",
    "            # достаем предыдущую биграму из beam.sequence\n",
    "            last_id = bigr2id[' '.join(beam.sequence[-2:])]\n",
    "            \n",
    "            # посмотрим вероятности продолжений для предыдущего слова\n",
    "            probas = matrix[last_id].toarray()[0]\n",
    "            \n",
    "            # возьмем топ самых вероятных продолжений\n",
    "            top_idxs = probas.argsort()[:-(max_beams+1):-1]\n",
    "            for top_id in top_idxs:\n",
    "                # иногда вероятности будут нулевые, такое не добавляем\n",
    "                if not probas[top_id]:\n",
    "                    break\n",
    "                \n",
    "                # создадим новый луч на основе текущего и варианта продолжения\n",
    "                new_sequence = beam.sequence + [id2word[top_id]]\n",
    "                # скор каждого луча это произведение вероятностей (или сумма логарифмов)\n",
    "                new_score = beam.score + np.log1p(probas[top_id])\n",
    "                new_beam = Beam(sequence=new_sequence, score=new_score)\n",
    "                new_beams.append(new_beam)\n",
    "        # отсортируем лучи по скору и возьмем только топ max_beams\n",
    "        beams = sorted(new_beams, key=lambda x: x.score, reverse=True)[:max_beams]\n",
    "    \n",
    "    # в конце возвращаем самый вероятный луч\n",
    "    best_sequence = max(beams, key=lambda x: x.score).sequence\n",
    "\n",
    "    \n",
    "    return ' '.join(best_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1a898d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start0> <start> как сообщает риа новости со ссылкой на источники в правоохранительных органах города интерфаксу сообщили в штабе объединенной группировки войск на северном кавказе генерал-полковник виктор казанцев может получить лишь иностранный гражданин который передал этим летом разразилась похожая эпидемия по данным риа новости со ссылкой на источники в правоохранительных органах города интерфаксу сообщили в штабе объединенной группировки войск на северном кавказе генерал-полковник виктор казанцев может получить лишь иностранный гражданин который передал этим летом разразилась похожая эпидемия по данным риа новости со ссылкой на источники в правоохранительных органах города интерфаксу сообщили в штабе объединенной группировки войск на северном кавказе генерал-полковник виктор казанцев <end>\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_news, bigr2id_news, id2word_news, word2id_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "745c461b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "куда пошли деньги мвф для выкупа государственных долгов руководствуясь особенностями ситуации данной страны fr во франции насчитывается более 33 миллионов зараженных вирусом спида в лидеры уверенно вышли республики бывшего ссср лондонскому клубу банков-кредиторов завершился существенным прогрессом но одновременно и противникам ку-клукс-клана было отказано во въезде в старинный русский город появились металлические цилиндры напоминающие поставленные на попа автоцистерны сообщает фан около 2 часов по московскому утра по местному телеканалу ичкерия с заявлением и патриарх московский и всея руси алексия ii не состоится поскольку жители дагестана заявляли что игорь иванов передает риа новости со ссылкой на источники в правоохранительных органах города интерфаксу сообщили в\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_news, bigr2id_news, id2word_news, word2id_news, start = 'куда пошли'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2b445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
